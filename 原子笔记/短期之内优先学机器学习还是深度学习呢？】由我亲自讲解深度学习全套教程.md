
# P2
[第一章：深度学习必备基础知识点\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=2&vd_source=40645413612f159eb5e31f3c39bc5f84)

[00:59](https://www.bilibili.com/video/BV1xW4y1g7qG?p=2&vd_source=40645413612f159eb5e31f3c39bc5f84#t=59.999113)
DL就是机器学习一部分
不是评算法, 更多的是数据层面, 特征是非常非常重要的, 什么特征更适合模型, 这比算法重要

[02:11](https://www.bilibili.com/video/BV1xW4y1g7qG?p=2&vd_source=40645413612f159eb5e31f3c39bc5f84#t=131.384242)
流程: 数据获取, 特征工程(最核心), 建模没什么难度, 评估和应用


[03:10](https://www.bilibili.com/video/BV1xW4y1g7qG?p=2&vd_source=40645413612f159eb5e31f3c39bc5f84#t=190.857295)
和机器学习区别, 让算法基于数据, 直接让他自己提取特征

机器学习--人工实现算法

DL: 让网络自己学习什么特征组合合适等等, 解决了特征工程这个大问题

[[假阳性率（false positive rate）]]
[04:56](https://www.bilibili.com/video/BV1xW4y1g7qG?p=2&vd_source=40645413612f159eb5e31f3c39bc5f84#t=296.43366)
特征工程的作用: 预处理和特征提取最核心, 数据特征决定了模型的上限, 比算法参数更核心

深度学习: 学习什么样的特征是最合适的, 解决问题是怎么样去提取特征

# P5

[4-视觉任务中遇到的问题\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=5&vd_source=40645413612f159eb5e31f3c39bc5f84)


K-近邻, 分类问题, 看A点最近的K个数据, 谁的类别最多, 就把A点分为那个类别

算法细节: 距离的定义

[09:06](https://www.bilibili.com/video/BV1xW4y1g7qG?p=5&vd_source=40645413612f159eb5e31f3c39bc5f84#t=546.52816)

k近邻, 不能用来图像分类, 因为无法区分什么是背景和主题

---
# P6
[5-得分函数\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=6&vd_source=40645413612f159eb5e31f3c39bc5f84)


线性函数, 每个类别对应行, 各自的权重对应列

![[Pasted image 20230404150244.png]]

---
# P7 

[6-损失函数的作用\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=7&vd_source=40645413612f159eb5e31f3c39bc5f84)

[04:53](https://www.bilibili.com/video/BV1xW4y1g7qG?p=7&vd_source=40645413612f159eb5e31f3c39bc5f84#t=293.971841)
## 损失函数
- 神经网络可以做分类和回归, 网络结构不变, 只是损失函数变化
- 损失函数对应损失的大小, 当然越小越好, 可以增加容忍程度
- 损失函数当然是基于已知标签的

---
# P8 前向传播整体流程
[7-前向传播整体流程\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=8&vd_source=40645413612f159eb5e31f3c39bc5f84)

[02:14](https://www.bilibili.com/video/BV1xW4y1g7qG?p=8&vd_source=40645413612f159eb5e31f3c39bc5f84#t=134.00756)
**正则话惩罚项**: 只关于权重参数的函数, 防止过拟合

越强大的算法, 越容易过拟合!没有用


[04:16](https://www.bilibili.com/video/BV1xW4y1g7qG?p=8&vd_source=40645413612f159eb5e31f3c39bc5f84#t=256.434005)
![[Pasted image 20230404151950.png]]


得分值的大小已经可以去做决策了, 这里讨论的是如何训练.

通过得分值做操作, 得到概率值, 再去得到损失函数, 让损失函数最小


[06:25](https://www.bilibili.com/video/BV1xW4y1g7qG?p=8&vd_source=40645413612f159eb5e31f3c39bc5f84#t=385.102057)
- sigmoid函数, 映射到0~1

- **交叉熵损失函数**! 利用**负log函数**, 只关心属于正确类别的概率, 概率越低, 损失越大!

- **softmax分类器**--把得分值换成概率, 再去算损失
	- 将得分取指数(扩大差距), 归一化, 然后对正确的类别取负对数

这就完成了前向传播, 计算完一个损失
[12:39](https://www.bilibili.com/video/BV1xW4y1g7qG?p=8&vd_source=40645413612f159eb5e31f3c39bc5f84#t=759.393086)
那反向传播呢, 就是去更新W, 损失高的时候, 去修改W, 用反向传播更新模型--梯度下降

---
# P9
[第二章：神经网络整体架构\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=9&vd_source=40645413612f159eb5e31f3c39bc5f84)


[06:05](https://www.bilibili.com/video/BV1xW4y1g7qG?p=9&vd_source=40645413612f159eb5e31f3c39bc5f84#t=365.39031)
一个小例子
[07:06](https://www.bilibili.com/video/BV1xW4y1g7qG?p=9&vd_source=40645413612f159eb5e31f3c39bc5f84#t=426.06986)
反向传播时是逐层计算, 逐层求偏导逐层修改参数让loss变小

---
# P10
[1-神经网络整体架构\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=10&vd_source=40645413612f159eb5e31f3c39bc5f84)


[00:20](https://www.bilibili.com/video/BV1xW4y1g7qG?p=10&vd_source=40645413612f159eb5e31f3c39bc5f84#t=20.869593)
小例子
![[Pasted image 20230404160647.png]]

常见门单元

[05:57](https://www.bilibili.com/video/BV1xW4y1g7qG?p=10&vd_source=40645413612f159eb5e31f3c39bc5f84#t=357.14901)

![[Pasted image 20230404161012.png]]


[09:02](https://www.bilibili.com/video/BV1xW4y1g7qG?p=10&vd_source=40645413612f159eb5e31f3c39bc5f84#t=542.568349)

![[Pasted image 20230404161445.png]]

input层的圈圈个数, 就是输入的列特征向量维度

---
[2-神经网络架构细节\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=11&vd_source=40645413612f159eb5e31f3c39bc5f84)

全连接
[00:37](https://www.bilibili.com/video/BV1xW4y1g7qG?p=11&vd_source=40645413612f159eb5e31f3c39bc5f84#t=37.659613)
hidden层的4个神经元, 就是4个有3个自变量的函数! 计算出来4个结果 


[07:36](https://www.bilibili.com/video/BV1xW4y1g7qG?p=11&vd_source=40645413612f159eb5e31f3c39bc5f84#t=456.130902)
层与层之间就是矩阵乘法, 中间要隔上非线性变换


[10:39](https://www.bilibili.com/video/BV1xW4y1g7qG?p=11&vd_source=40645413612f159eb5e31f3c39bc5f84#t=639.205938)
反复堆叠, 参数极多就是强大之处

---
# P12
[5-正则化与激活函数\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=12&vd_source=40645413612f159eb5e31f3c39bc5f84)

[01:37](https://www.bilibili.com/video/BV1xW4y1g7qG?p=12&vd_source=40645413612f159eb5e31f3c39bc5f84#t=97.7987)
参数是一组一组去加的

[03:46](https://.bilibili.com/video/BV1xW4y1g7qG?p=12&vd_source=40645413612f159eb5e31f3c39bc5f84#t=226.510239)
避免过拟合
![[Pasted image 20230404163443.png]]

[05:00](https://www.bilibili.com/video/BV1xW4y1g7qG?p=12&vd_source=40645413612f159eb5e31f3c39bc5f84#t=276.014677)

![[Pasted image 20230404163533.png]]


[05:43](https://www.bilibili.com/video/BV1xW4y1g7qG?p=12&vd_source=40645413612f159eb5e31f3c39bc5f84#t=343.340712)
非线性激活函数

sigmoid已经不用了, 反向传播时, 求导之后为0, 梯度消失现象
90%都是relu函数

---
# P13
[6-神经网络过拟合解决方法\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=13&vd_source=40645413612f159eb5e31f3c39bc5f84)


[00:02](https://www.bilibili.com/video/BV1xW4y1g7qG?p=13&vd_source=40645413612f159eb5e31f3c39bc5f84#t=2.50412)
数据预处理
中心化, 维度拉伸

[02:50](https://www.bilibili.com/video/BV1xW4y1g7qG?p=13&vd_source=40645413612f159eb5e31f3c39bc5f84#t=170.427401)

参数初始化也很重要!一般随机生成矩阵\*0.01
我们尽可能想让参数矩阵相差比较小

[03:52](https://www.bilibili.com/video/BV1xW4y1g7qG?p=13&vd_source=40645413612f159eb5e31f3c39bc5f84#t=232.728269)

drop-out(七伤拳, 防止过拟合)
![[Pasted image 20230404164449.png]]
在参数传播的过程中, **随机地选择**一些神经元, 此次传播他们对应的值不变

测试的时候, 就没有这些×了, 直接全用

---
[第三章：卷积神经网络原理与参数解读\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=14&vd_source=40645413612f159eb5e31f3c39bc5f84)

就讲了一些应用
GPU做卷积比cpu快多了

卷积的特征提取方法, 这些神经网络都是去提特征

---
[2-卷积的作用\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=15&vd_source=40645413612f159eb5e31f3c39bc5f84)


[00:02](https://www.bilibili.com/video/BV1xW4y1g7qG?p=15&vd_source=40645413612f159eb5e31f3c39bc5f84#t=2.387311)
cnn与传统的区别: cnn直接输入原始数据
传统的维度太大了
3维的

[02:53](https://www.bilibili.com/video/BV1xW4y1g7qG?p=15&vd_source=40645413612f159eb5e31f3c39bc5f84#t=173.610435)
卷积层, 池化层, 全连接层


[04:53](https://www.bilibili.com/video/BV1xW4y1g7qG?p=15&vd_source=40645413612f159eb5e31f3c39bc5f84#t=293.721278)

---
[3-卷积特征值计算方法\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=16&vd_source=40645413612f159eb5e31f3c39bc5f84)


[05:05](https://www.bilibili.com/video/BV1xW4y1g7qG?p=16&vd_source=40645413612f159eb5e31f3c39bc5f84#t=305.193033)
卷积的实现过程
**卷积核的深度要和输入的深度相同!**
1个卷积核, 得到一个结果!

[07:31](https://www.bilibili.com/video/BV1xW4y1g7qG?p=16&vd_source=40645413612f159eb5e31f3c39bc5f84#t=451.053657)

是卷积核**分别**和不同的通道计算!算完相加, 再加一个偏置

---
![[Pasted image 20230404173438.png]]

[7-特征图尺寸计算与参数共享\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=20&vd_source=40645413612f159eb5e31f3c39bc5f84)

**卷积参数共享**
[03:39](https://www.bilibili.com/video/BV1xW4y1g7qG?p=20&vd_source=40645413612f159eb5e31f3c39bc5f84#t=219.670007)
用同样的卷积核, 对不同的小区域进行卷积

[06:13](https://www.bilibili.com/video/BV1xW4y1g7qG?p=20&vd_source=40645413612f159eb5e31f3c39bc5f84#t=373.590324)
参数比全连接层少多了
一般两次卷积, 一次池化

---
[9-整体网络架构\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=22&vd_source=40645413612f159eb5e31f3c39bc5f84)


[04:39](https://www.bilibili.com/video/BV1xW4y1g7qG?p=22&vd_source=40645413612f159eb5e31f3c39bc5f84#t=279.556805)
CNN之后拉长, 进入全连接层!(全连接层只算1层)
---
[10-VGG网络架构\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=23&vd_source=40645413612f159eb5e31f3c39bc5f84)


[05:31](https://www.bilibili.com/video/BV1xW4y1g7qG?p=23&vd_source=40645413612f159eb5e31f3c39bc5f84#t=331.554939)
层数不是越多越好, 16比30的好

---
[11-残差网络Resnet\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=24&vd_source=40645413612f159eb5e31f3c39bc5f84)


[04:10](https://www.bilibili.com/video/BV1xW4y1g7qG?p=24&vd_source=40645413612f159eb5e31f3c39bc5f84#t=250.958175)
resnet网络, 主流, 让网络上限提升
当成一个特征提取就好了
---
[12-感受野的作用\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=25&vd_source=40645413612f159eb5e31f3c39bc5f84)
感受野是关于一个点定义的, 反推到第一层
感受野越大越好


[03:09](https://www.bilibili.com/video/BV1xW4y1g7qG?p=25&vd_source=40645413612f159eb5e31f3c39bc5f84#t=189.786687)

为什么要用3个3\*3而不是一个7\*7呢
原因: 参数更少!非线性变换也增多
---
[第四章：递归神经网络与词向量原理解读\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=26&vd_source=40645413612f159eb5e31f3c39bc5f84)
在传统基础上改良而已

[01:20](https://www.bilibili.com/video/BV1xW4y1g7qG?p=26&vd_source=40645413612f159eb5e31f3c39bc5f84#t=80.684881)
传统神经网络, 时间序列上不同的两个输入向量没有交集

递归伸进网络, 前面生成的特征再输回来, 进行一些交叠, 可以寻找在时间上的相关性

[06:42](https://www.bilibili.com/video/BV1xW4y1g7qG?p=26&vd_source=40645413612f159eb5e31f3c39bc5f84#t=402.894446)

辅助理解
![[Pasted image 20230404191929.png]]

**LSTM--long short term memory**
![[Pasted image 20230404192707.png]]
RNN会把之前所有的结果联系起来, 时长太长就不精了

[09:18](https://www.bilibili.com/video/BV1xW4y1g7qG?p=26&vd_source=40645413612f159eb5e31f3c39bc5f84#t=558.796875)

LSTM--可以去忘记一些特征
控制参数C, 控制模型复杂度

---
[第四章：递归神经网络与词向量原理解读\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1xW4y1g7qG?p=27&vd_source=40645413612f159eb5e31f3c39bc5f84)
自然语言处理--wordvector

[GitHub - esm7/obsidian-vimrc-support: A plugin for the Obsidian.md note-taking software](https://github.com/esm7/obsidian-vimrc-support#some-help-with-binding-space-chords-doom-and-spacemacs-fans)


#知识/机器学习/深度学习